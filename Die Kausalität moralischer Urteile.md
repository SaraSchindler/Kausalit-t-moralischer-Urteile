---
title: Die Kausalität moralischer Urteile
---
Wörter: 5000-6000



# Einleitung

Zentrale Fragen: Wie fällen wir moralische Urteile? Sind unsere moralischen Urteile aufgrund der Mechanismen, durch die wir sie fällen, für unzuverlässig zu erklären? Welche Auswirkungen haben neurowissenschaftliche Erklärungen unserer moralischen Urteile auf Moraltheorien? Stützen sie bestimmte Ansätze und widerlegen sie andere? Sollten neurowissenschaftliche Ergebnisse überhaupt in die philosophische Debatte einfliessen?

# Ergebnisse aus den Neurowissenschaften


## Biologische und kulturelle Ursprünge  moralischen Urteilens

Eine grosse Frage, die sich in Bezug auf unsere Fähigkeit, moralische Urteile zu fällen, stellt, ist die Frage nach dem Ursprung dieser Fähigkeit. Sind moralische Urteile in angeborenen Eigenschaften verwurzelt oder lassen sie sich vollständig durch erworbene kulturelle Eigenschaften erklären? Im 20. Jahrhundert fielen die Antworten der Wissenschaftler/innen auf diese Frage eher extrem aus. Während der Psychologe B. F. Skinner [-@skinnerFreedomDignity1971]  moralische Regeln als sozial bedingte Verhaltensmuster sah und der Meinung war, dass man Menschen dazu bringen könne, so ziemlich alles als moralisch richtig oder falsch zu beurteilen, wenn man nur die richtigen Verstärkungen gebrauchte, war der Biologe E. O. Wilson [-@wilsonSociobiologyNewSynthesis2000] der Ansicht, dass fast die gesamte menschliche Moral durch die Anwendung der Evolutionsbiologie verstanden werden könne. Zu Beginn des 21. Jahrhunderts begannen jedoch die meisten Wissenschaftler und Wissenschaftlerinnen auf dem Gebiet des moralischen Urteilens ein hybrides Modell zu bevorzugen, welches sowohl biologischen als auch kulturellen Faktoren eine Rolle zuweist [vgl. @riniMoralityCognitiveScience2015].

Ein Beispiel für einen solchen modernen Ansatz ist die *Moral Foundations theory* vom Psychologen Jonathan Haidt [-@haidtRighteousMindWhy2012]. Haidt hebt die Rolle kultureller Unterschiede bei der wissenschaftlichen Untersuchung von Moralität hervor. Ihm zufolge lassen sich alle dokumentierten moralischen Überzeugungen in eine Handvoll moralischer Unterbereiche einordnen, wie beispielsweise Schadensvermeidung, Gerechtigkeit (bei der Verteilung von Ressourcen), Respekt vor Autorität und Reinheit. Haidt argumentiert, dass moralische Unterschiede zwischen den Kulturen Unterschiede in der Gewichtung dieser Grundlagen widerspiegeln. Eine Kultur, die alle moralischen Intuitionen, die der menschliche Geist zu erfahren bereit ist, gleich gewichtet, würde Handlungsunfähigkeit riskieren, da jede Handlung mehrere widersprüchliche Intuitionen auslöst. Wenn ein Kind geboren wird, ist es bereit, in allen Bereichen moralische Intuitionen zu entwickeln, aber sein lokales kulturelles Umfeld betont im Allgemeinen nur ein oder zwei dieser Ethiken. Intuitionen innerhalb einer kulturell gewichteten Ethik werden stärker ausgebildet, während Intuitionen innerhalb einer nicht gewichteten Ethik schwächer ausgeprägt sind [vgl. @haidtEmotionalDogIts2001,827]. 
Solche *Maintenance-Loss-Modelle* sind auch in anderen Bereichen der menschlichen höheren Kognition dokumentiert worden. Es scheint ein Konstruktionsmerkmal von Säugetiergehirnen zu sein, dass ein Grossteil der neuronalen Entwicklung 'erfahrungserwartend' ist [vgl. @haidtEmotionalDogIts2001, 827] . Das heißt, es gibt entwicklungszeitlich festgelegte Perioden hoher neuraler Plastizität, als ob das Gehirn zu einem bestimmten Zeitpunkt bestimmte Arten von Erfahrung 'erwartet', um seine endgültige Verdrahtung zu steuern. Solche sensiblen Perioden sind in der Entwicklung der sensorischen Systeme [vgl. @hubelPeriodSusceptibilityPhysiological1970] und der Sprache [vgl. @johnsonCriticalPeriodEffects1989] gut dokumentiert.
Haidt besteht aber auch darauf, dass die Biologie eine wichtige Rolle bei der Erklärung moralischer Urteile spielt: er sieht jeden dieser moralischen Unterbereiche in einem bestimmten evolutionären Ursprung verwurzelt. Bereits 2001 schrieb er:

> It [das soziale intuitionistische Modell, welches Haidt vertritt] proposes that morality, like language, is a major evolutionary adaptation for an intensely social species, built into multiple regions of the brain and body, that is better described as emergent than as learned yet that requires input and shaping from a particular culture. Moral intuitions are therefore both innate and enculturated [@haidtEmotionalDogIts2001,826].

Haidts Theorie ist nach wie vor ziemlich umstritten, aber sie ist ein prominentes Beispiel dafür, dass zeitgenössische Wissenschaftler/innen sich darauf konzentrieren, polarisierte Antworten auf die Frage 'Biologie oder Kultur' zu vermeiden [vgl. @riniMoralityCognitiveScience2015].

## Rationale Überlegungen und Intuitionen in moralischen Urteilen

Eine entscheidende Frage ist, ob moralische Urteile aus bewussten Überlegungen und Reflexionen entstehen oder durch unbewusste und unmittelbare Impulse ausgelöst werden. In den 1970er und 1980er Jahren wurde die Forschung in der Moralpsychologie durch die Arbeiten von Lawrence Kohlberg dominiert, der eine stark rationalistische Konzeption des moralischen Urteilens vertrat  [vgl. @riniMoralityCognitiveScience2015]. Seit der Wende zum zwanzigsten Jahrhundert hat man sich aber von dieser Ansicht gelöst. Ein Wendepunkt war die Veröffentlichung von Jonathan Haidts Arbeit _The Emotional Dog and Its Rational Tail_ [-@haidtEmotionalDogIts2001]. Dort erörtert Haidt ein Phänomen, das er als _moral dumbfounding_ bezeichnet. Er stellte seinen Probanden provokative Geschichten vor, wie beispielsweise die folgende Geschichte von einem Bruder und einer Schwester, die bewusst Inzest betreiben:
> Julie and Mark are brother and sister. They are traveling together in France on summer vacation from college. One night they are staying alone in a cabin near the beach. They decide that it would be interesting and fun if they tried making love. At the very least it would be a new experience for each of them. Julie was already taking birth control pills, but Mark uses a condom too, just to be safe. They both enjoy making love, but they decide not to do it again. They keep that night as a special secret, which makes them feel even closer to each other. What do you think about that? Was it OK for them to make love? [@haidtEmotionalDogIts2001,814] 

Als die Probanden gebeten wurden, ihre moralischen Urteile zu dieser Geschichte zu erklären, nannten sie Gründe, die durch die Beschreibung der Geschichten ausgeschlossen zu sein scheinen - so sagten sie zum Beispiel, dass die inzestuösen Geschwister ein Kind mit gefährlichen Geburtsfehlern erzeugen könnten, obwohl die Geschichte deutlich macht, dass sie sehr vorsichtig waren und sogar doppelt verhüteten, um eine Empfängnis zu vermeiden. Als sie an diese Details erinnert wurden, revidierten die Probanden ihre moralischen Urteile nicht, sondern sie sagten Dinge wie: *'Ich weiss nicht, warum es falsch ist, es ist einfach so'*. Haidt sieht solche Aussagen als Beweis dafür, dass das rationalistische Modell moralischen Urteilens nicht zutreffen kann: wie sollte es erklären, dass jemand weiss, dass eine Handlung moralisch falsch ist, aber nicht warum, wenn moralische Urteile auf rationalen Überlegungen basieren? Haidt schlägt daher ein alternatives Modell vor: das sozial intuitionistische Modell [vgl. @haidtEmotionalDogIts2001, 814]. Dieses Modell besagt, dass wir moralische Urteile nicht durch rationale Überlegungen fällen, sondern die rationalen Überlegungen lediglich unser bereits gefälltes moralisches Urteil rationalisieren sollen. Das Modell ist insofern ein soziales Modell, als es die private Argumentation der Individuen in den Hintergrund rückt und stattdessen die Bedeutung sozialer und kultureller Einflüsse betont. Das Modell ist insofern ein intuitionistisches Modell, als es festhält, dass das moralische Urteil im Allgemeinen das Ergebnis von schnellen, automatischen Bewertungen (Intuitionen) ist [vgl. @haidtEmotionalDogIts2001,814]. Obwohl beim sozial intuitionistischen Modell der Fokus klar auf moralischen Intuitionen liegt, räumt Haidt rationalen, moralischen Überlegungen doch auch einen Platz ein. Folgende drei Funktionen würden rationale Überlegungen erfüllen [vgl. @haidtEmotionalDogIts2001,828-829]:

- Die erste Funktion ist, dass die (ex post facto) moralische Überlegung von Menschen eine kausale Wirkung haben kann - auf die Intuitionen anderer Menschen. Aus der Sicht der sozialen Intuitionisten ist das moralische Urteilsvermögen nicht nur eine einzelne Handlung, die im Verstand einer einzelnen Person stattfindet, sondern ein fortlaufender Prozess, der sich oft über einen längeren Zeitraum und über mehrere Personen erstreckt. Begründungen und Argumente können zirkulieren und Menschen beeinflussen, auch wenn sich Einzelne nur selten mit privaten moralischen Überlegungen beschäftigen.
- Die zweite Funktion ermöglicht es, dass Menschen manchmal private moralische Überlegungen für sich selbst anstellen können, insbesondere wenn ihre anfänglichen Intuitionen in Konflikt geraten. Abtreibung mag sich für viele Menschen falsch anfühlen, wenn sie an den Fötus denken, aber richtig, wenn sie ihre Aufmerksamkeit auf die Frau verlagern. Wenn konkurrierende Intuitionen gleich stark sind, gerät das Beurteilungssystem in eine Sackgasse. Unter solchen Umständen verwendet man Argumentation und Intuition zusammen, um die Blockade zu durchbrechen. Das heisst, wenn man ein Dilemma bewusst untersucht und sich dabei abwechselnd auf jede beteiligte Partei konzentriert, werden verschiedene Intuitionen ausgelöst, die zu verschiedenen widersprüchlichen Urteilen führen. Mit Hilfe der Vernunft kann dann für jedes Urteil eine Argumentation konstruiert werden. Wenn die Argumentation für eines der Urteile besser ist als für die anderen, wird sich das Urteil richtig anfühlen. #Kommentar: RE?
- Die dritte Funktion besteht darin, dass eine Person im Prinzip einfach durch reine rationale Überlegungen zu einem Urteil gelangen könnte, das ihrer ursprünglichen Intuition widerspricht. Haidt zufolge geschieht dies aber höchst selten. Die Tatsache, dass es dennoch vorkommt, reicht aber bereits aus, um zu zeigen, dass das reine moralische Schlussfolgern eine kausale Rolle beim Fällen moralischer Urteile spielen kann.

## Die Kausalstruktur moralischer Urteile

Wir können moralische Urteile auch auf ihre Kausalstruktur untersuchen.  Welche Faktoren sind psychologisch gesehen ausschlaggebend, um ein bestimmtes moralisches Urteil über einen bestimmten Fall zu fällen? Sind dies dieselben Faktoren, auf die sich Moralphilosophen berufen, wenn sie ethische Entscheidungen formell analysieren? Die empirische Forschung scheint etwas anderes zu suggerieren.

Beispielsweise scheint Absichtlichkeit ein wichtiger Faktor zu sein, wenn wir die Moralität einer Handlung beurteilen. Die meisten Philosophen sind davon ausgegangen, dass etwas, das eine Person getan hat, nur dann als moralisch richtig oder falsch bewertet werden kann, wenn die Person vorsätzlich gehandelt hat (zumindest in gewöhnlichen Fällen, wobei Fahrlässigkeit ausser Acht gelassen werden muss). Wenn Peter Anna absichtlich stolpern lässt, dann ist diese Handlung moralisch falsch, wenn Peter hingegen Anna versehentlich stolpern lässt, dann ist das lediglich bedauerlich. Es scheint also, die Beurteilung von Absicht müsse kausal vor der Beurteilung der Moralität einer Handlung stattfinden.  Das heisst, wenn ich eine potenziell moralisch relevante Situation beurteile, stelle ich zunächst fest, ob die betroffene Person absichtlich gehandelt hat, und dann nutze ich dieses Urteil als Input, um herauszufinden, ob das, was sie getan hat, moralisch falsch ist. 
Empirische Belege legen jedoch nahe, dass dieses einfache Modell falsch ist.  Eine bekannte Reihe von Studien über den Nebenwirkungseffekt (auch bekannt als der Knobe-Effekt, nach seinem Entdecker Joshua Knobe) scheint zu zeigen, dass die kausale Beziehung zwischen moralischem Urteil und Absichtsurteil viel komplizierter ist. Joshua Knobe liess seine Probanden eine Kurzgeschichte wie die folgende lesen: 

> The vice-president of a company went to the chairman of the board and said, “We are thinking of starting a new program. It will help us increase profits, but it will also harm the environment.” The chairman of the board answered, “I don’t care at all about harming the environment. I just want to make as much profit as I can. Let’s start the new program.” They started the new program. Sure enough, the environment was harmed [@knobeIntentionalActionSide2003,191].

Andere Teilnehmer lasen die gleiche Geschichte, nur dass das Programm als Nebeneffekt der Umwelt eher _helfen_ als schaden würde. Beide Teilnehmergruppen wurden gefragt, ob die Führungskraft den Nebeneffekt absichtlich herbeigeführt habe. Auffallend war, dass 82% der Probanden dachten, dass der Nebeneffekt absichtlich herbeigeführt wurde, wenn die Nebenwirkung moralisch falsch war (die Umwelt zu schädigen), aber wenn die Nebenwirkung moralisch gut war (der Umwelt zu helfen), dachten 77% der Probanden, dass der Nebeneffekt _nicht_ absichtlich herbeigeführt wurde [vgl. @knobeIntentionalActionSide2003,192]. Dieses Experiment (die Befunde wurden in vielen anderen Studien repliziert) legt nahe, dass die kausale Beziehung zwischen der Bewertung der Absichtlichkeit und dem moralischen Urteil nicht unidirektional ist. Menschen fällen manchmal moralische Urteile, bevor sie die Absichtlichkeit beurteilen. Anstatt dass die Beurteilung der Absichtlichkeit immer ein Input für die moralische Beurteilung ist, ist die moralische Beurteilung manchmal ein Input für die Beurteilung der Absichtlichkeit. Eine Nebenwirkung, die als falsch beurteilt wird, wird mit grösserer Wahrscheinlichkeit als absichtlich beurteilt als eine, die als moralisch richtig beurteilt wird.

#Kommentar: evtl. hier noch etwas ausbauen? Es geht ja hier um die Kausalität moralischer Urteile

## Die Neuroanatomie moralischer Urteile

Eine weitere interessante Frage ist, welche Gehirnareale beim Fällen moralischer Urteile aktiv sind. Eine äusserst einflussreiche Studie zum moralischen Urteilen wurde von Joshua D. Greene und Kollegen durchgeführt. In dieser Arbeit wurde die funktionelle Magnetresonanztomographie (fMRI) eingesetzt, bei der ein starker Magnet verwendet wird, um eine visuelle Darstellung der relativen Niveaus der in verschiedenen Hirnarealen verwendeten zellulären Energie zu erzeugen. Der Einsatz der fMRI ermöglicht es den Forschern und Forscherinnen, eine nahezu Echtzeit-Darstellung der Gehirnaktivitäten zu erhalten, während das bewusste Subjekt Urteile über moralisch relevante Szenarien fällt. Die Studie von Greene et al. zeigt, dass wir bei moralischen Urteilen oft intuitiv entscheiden. Diese Studie befasste sich mit einem Thema, das Philosophen schon länger beschäftigt: mit den Trolley-Dilemmata, genauer mit dem Standard Trolley-Problem und der Footbridge-Variante davon. Beim Standard Trolley-Problem steht jemand an einem Glei und bemerkt, dass ein führerloser Trolley das Gleis hinunterrollt und auf eine Gruppe von fünf Personen zufährt. Diese Personen werden alle getötet, wenn der Trolley auf seinem Weg weiterfährt. Die Person hat nun aber die Möglichkeit, eine Weiche umzustellen, die den Trolley auf ein Nebengleis umleitet, auf dem sich aber auch eine Person befindet, welche getötet wird, wenn die Weiche umgestellt wird. Auf die Frage, was diese Person tun soll, antworten die meisten Menschen, dass sie den Trolley auf das Nebengleis umleiten soll und so diese fünf Menschen auf dem Hauptgleis auf Kosten der einen Person auf dem Nebengleis retten soll. Bei der Footbridge-Variante des Trolley-Problems geht es ebenfalls um einen Trolley, der droht, fünf Menschen auf dem Gleis zu töten, diesmal steht die Person aber nicht neben einer Weiche, sondern auf einer Fussgängerbrücke, die über dem Gleis verläuft. Die Person hat keine Möglichkeit, den Trolley irgendwie umzuleiten. Sie erwägt kurz, von der Brücke vor den Trolley zu springen und sich zu opfern, um so die anderen fünf Personen zu retten, kommt aber schnell zum Schluss, dass sie selber viel zu leicht ist, um den Trolley aufzuhalten: sie würde nur mit den anderen Personen sterben. Neben ihr steht aber ein grosser und dicker Fremder. Die einzige Möglichkeit, den Trolley aufzuhalten und damit die fünf Menschen zu retten, ist es, den Fremden von der Brücke vor den Trolley zu stossen, was den Fremden töten wird. Auf die Frage, was die Person in dieser Situation tun sollte, sagen die meisten Menschen, dass sie den Fremden *nicht* von der Brücke stossen sollte.

Die von Greene und seinen Kollegen durchgeführte Studie zielt nun darauf ab, zwei Fragen zu beantworten [vgl. @greeneFMRIInvestigationEmotional2001, 2105-2106]:

1. Was macht es moralisch akzeptabel, im Trolley-Dilemma ein Leben zu opfern, um fünf zu retten, aber nicht im Footbridge-Dilemma?

2. Wie kommt es, dass die meisten zur Schlussfolgerung gelangen, dass es akzeptabel ist, im Trolley-Dilemma ein Leben für fünf zu opfern, nicht aber im Footbridge-Dilemma, obwohl eine zufriedenstellende Rechtfertigung für die _Unterscheidung zwischen diesen beiden Fällen_ bemerkenswert schwierig zu finden ist?

Um diesen Fragen nachzugehen, haben sie zwei fMRI-Studien durchgeführt. Die Autoren vermuteten, dass der entscheidende Unterschied zwischen dem Trolley-Dilemma und dem Footbridge-Dilemma darin besteht, dass letzteres die Emotionen der Menschen auf eine Art und Weise involviert, wie es das erstere nicht tut. Der Gedanke, jemanden in den Tod zu stossen, ist emotional stärker geprägt als der Gedanke, einen Schalter zu betätigen und es ist diese emotionale Reaktion, die die Neigung der Menschen erklärt, diese Fälle unterschiedlich zu behandeln. Greene et al. stellen die allgemeinere Hypothese auf, dass einige moralische Dilemmata (solche, die dem Footbridge-Dilemma in relevantem Masse ähnlich sind) in grösserem Masse emotionale Verarbeitung erfordern als andere (solche, die dem Trolley-Dilemma in relevantem Masse ähnlich sind), und dass diese Unterschiede in der emotionalen Involviertheit sich auf die Urteile der Menschen auswirken. Die Autoren machen zwei Vorhersagen, die sie in ihrer Studie testen [vgl. @greeneFMRIInvestigationEmotional2001, 2106]:

1. Mit Emotionen verbundene Hirnareale (medialer frontaler Gyrus, hinterer zingulärer Gyrus und bilateraler STS) sind aktiver bei der Auseinandersetzung mit Dilemmata wie dem Footbridge-Dilemma im Vergleich zur Auseinandersetzung mit Dilemmata wie dem Trolley-Dilemma.
2. Es gibt ein Muster von _Verhaltensinterferenzen_ (ähnlich wie in kognitiven Aufgaben, in denen automatisierte Prozesse Reaktionen beeinflussen können, z.B. der Stroop task). Die Autoren behaupten, dass die Testpersonen eine starke, automatische emotionale Reaktion im Footbridge-Dilemma haben, die ihnen sagt, dass es nicht in Ordnung ist, einen Menschen von der Brücke zu stossen, um fünf andere zu retten. Die Autoren erwarten, dass diejenigen Testpersonen, die dennoch sagen, dass es in Ordnung ist, den dicken Mann von der Brücke zu stossen - entgegen ihrer emotionalen Reaktion – aufgrund dieser emotionalen Interferenz eine längere Reaktionszeit haben. Generell sagen sie längere Reaktionszeiten für Tests voraus, bei denen die Antwort des Probanden nicht mit der emotionalen Reaktion übereinstimmt (z.B. "angemessen" für ein Dilemma wie das Footbridge-Dilemma). Sie haben das Fehlen solcher Effekte für Dilemmata wie dem Trolley-Dilemma vorhergesagt, die nach ihrer Theorie mit geringerer Wahrscheinlichkeit eine starke emotionale Reaktion hervorrufen.

In den Experimenten wurden beide Vorhersagen bestätigt. Es wurden drei Arten von Dilemmata getestet [vgl. @greeneFMRIInvestigationEmotional2001, 2106]:

- **Nicht-moralisch**: z.B. z.B. Fisch mit Reis oder Kartoffeln servieren?
- **Moralisch-persönlich**: z.B. Footbridge-Dilemma; unerlaubtes Entnehmen von Organen, um sie an fünf andere Menschen zu verteilen; Leute von einem sinkenden Rettungsboot stossen
- **Moralisch-unpersönlich**: z.B. Trolley-Dilemma; das Geld behalten, das man in einem verlorenen Porte-Monnaie gefunden hat

Die Testpersonen mussten jeweils angeben, ob sie eine Handlung für angebracht oder unangebracht halten. Die Resultate der Experimente werden in den Abbildungen 1 und 2 dargestellt.

![Resultate von Experiment 1 (Greene et al. 2001: 2106)](b3e6859e58fc858f25aaaf4eab9b984e.png)

![Resultate von Experiment 2 (Greene et al. 2001: 2107)](8dd20ff66e9d3fae98c53acdd5765453.png)

Wie man erkennen kann, sind bei den moralisch-persönlichen Dilemmata diejenigen Gehirnareale, die mit Emotionen in Verbindung gebracht werden, signifikant aktiver als bei moralisch-unpersönlichen und nicht-moralischen Dilemmata. Zudem sind Bereiche, die mit dem Arbeitsgedächtnis assoziiert werden, während der emotionalen Verarbeitung weniger aktiv als während kognitiver Verarbeitung. In Experiment 2 zeigt sich, dass diejenigen Testpersonen, die in moralisch-persönlichen Dilemmata (wie dem Footbridge-Dilemma) urteilten, dass es ‘angebracht’ ist, den Fremden von der Brücke zu stossen, eine längere Reaktionszeit hatten als diejenigen, die in dieser Situation ‘unangebracht’ urteilten. Dies wird von den Autoren darauf zurückgeführt, dass sie ‘entgegen ihrer Intuitionen’ entscheiden, d.h. dass auch diese Personen ganz automatisch die Intuition haben, dass es unangebracht ist, den Fremden vor den Trolley zu stossen und diese Intuition dann aber mit rationaler Überlegung ‘überwinden’, was Zeit kostet [vgl. @greeneFMRIInvestigationEmotional2001, 2106-2107]. Sehr oft wird argumentiert, dass im Trolley-Dilemma die meisten Personen ein utilitaristisches Urteil fällen, während die Mehrheit der Probanden im Footbridge-Dilemma ein deontologisches Urteil fällt. Entsprechend werden die Resultate dieser Studie dann so gedeutet, dass utilitaristische Urteile durch rationale Überlegungen gefällt werden, während deontologische Urtiele ihren Ursprung in der Aktivität von Gehirnarealen haben, die mit Emotionen assoziiert werden [vgl. @tennantHowWeMake2015]. Einige Philosophen [vgl. z.B. @singerEthicsIntuitions2005] argumentieren auf dieser Basis, dass moralische Intuitionen nicht verlässlich sind und es daher nicht zulässig ist, eine Theorie (z.B. den Präferenzutilitarismus, wie ihn Singer vertritt) aufgrund kontraintuitiver Aussagen zu verwerfen. Hierbei stimmt ihm auch Greene selbst zu, der in diesen psychologischen Fakten einen Grund sieht, unseren deontologischen Urteilen zu misstrauen [vgl. @greenePointandShootMoralityWhy2014]. 

# Die Auswirkungen dieser Ergebnisse der Neurowissenschaften auf Moraltheorien

Die Neurowissenschaften liefern uns also Informationen darüber, wie unsere moralischen Urteile kausal zu Stande kommen. Aber die Philosophen und Philosophinnen sind sich nicht einig darüber, was man von diesen Informationen halten soll. Einige sind der Meinung, dass diese Ergebnisse der Neurowissenschaften für philosophische Theorien völlig irrelevant sind. Andere hingegen sehen für solche neurowissenschaftlichen Ergebnisse durchaus einen Platz in der Philosophie. In diesem Kapitel sollen unterschiedliche Argumente für beide Seiten untersucht werden.

## Moralische Kognition und der Sein-Sollens-Fehlschluss

Wir können grundsätzlich zwei Arten von Aussagen unterscheiden: deskriptive Sätze, die beschreiben, wie etwas *ist* (z.B. *'Die Katze sass auf dem Balkon'*) und präskriptive Sätze, die sagen, wie etwas sein *soll* oder *nicht sein soll* (z.B. *'Es war falsch, die Katze vom Balkon zu stossen'*). Es ist sehr wichtig, diese beiden Arten von Aussagen klar zu trennen, denn wenn man dies nicht tut, so läuft man Gefahr, einen Sein-Sollens-Fehlschluss zu begehen. Dabei schliesst man von einer deskriptiven Aussage auf eine präskriptive Aussage, was jedoch nicht ohne weiteres zulässig ist [vgl. @humeTreatiseHumanNature2009]. Nur weil etwas ist, wie es ist, heisst das nicht, dass es auch so sein soll.

Kognitionswissenschaftliche Erkenntnisse sind Aussagen des Sein-Typs; sie beschreiben Fakten darüber, wie unser Verstand tatsächlich arbeitet, und nicht darüber, wie er arbeiten sollte. Uns interessieren hier aber eine besondere Art von deskriptiven Aussagen: nämlich deskriptive Aussagen über den Ursprung präskriptiver Urteile, d.h. Sein-Aussagen über den Ursprung von Sollens-Aussagen. Einige Philosophen und Philosophinnen argumentieren nun, dass Versuche, moralische Überzeugungen auf der Grundlage kognitionswissenschaftlicher Erkenntnisse zu ändern, auf Verwirrungen und Verwechslungen von Sein- und Sollens-Aussagen basieren und daher nicht zulässig sind. Sie behaupten, es sei eine Art Kategorienfehler, unsere Überzeugungen in einem präskriptiven Bereich entscheidend von Aussagen in einem deskriptiven Bereich abhängig zu machen oder sie gar deshalb zu revidieren [vgl. @riniMoralityCognitiveScience2015].
Natürlich ist damit nicht gemeint, dass es immer falsch ist, moralische Überzeugungen auf der Grundlage neuer wissenschaftlicher Informationen anzupassen. Stellen Sie sich vor, Sie sind Chef einer Sprengmannschaft und stehen kurz davor, den Auslöser zu drücken, um eine alte Fabrik in die Luft zu sprengen. Plötzlich schreit eines Ihrer Besatzungsmitglieder: _"Warten Sie, schauen Sie auf den Wärmemonitor! Da ist eine Wärmesignatur im Inneren der Fabrik - das ist wahrscheinlich eine Person! Sie sollten den Auslöser nicht drücken!"_ Es wäre äusserst unpassend, wenn Sie antworten würden, dass die Entscheidung, ob Sie den Auslöser drücken sollen oder nicht, nicht von den Erkenntnissen wissenschaftlicher Vorrichtungen wie Wärmemonitoren abhängen kann  [vgl. @riniMoralityCognitiveScience2015]. Es kann also nicht darum gehen, _alle_ empirischen Informationen von moralischen Überlegungen auszuschliessen. Aber es ist wichtig zu beachten, dass das wissenschaftliche Instrument selbst uns nicht sagt, was wir tun sollen. Wir können nicht einfach moralische Schlussfolgerungen aus deskriptiven wissenschaftlichen Fakten ablesen. Wir brauchen eine Art Brückenprämisse; etwas, das die rein deskriptive Aussage mit einer präskriptiven Aussage verbindet. Im Fall der Sprengmannschaften könnte eine solche Brückenprämisse etwa eine solche Form haben: _'Wenn das Drücken eines Auslösers den gewaltsamen Tod einer unschuldigen Person verursacht, dann sollte man den Auslöser nicht drücken.'_ Bei alltäglichen moralischen Interaktionen bleiben die Brückenprämissen oft implizit - es ist für jede beteiligte Person in diesem Szenario offensichtlich, dass die Anwesenheit eines unschuldigen Menschen die Falschheit der geplanten Sprengung impliziert [vgl. @riniMoralityCognitiveScience2015].
Aber es besteht ein Risiko, wenn wir unsere Brückenprämissen implizit belassen. Manchmal scheint man sich auf implizite Brückenprämissen zu verlassen, über die man sich nicht einig ist oder die vielleicht gar keinen Sinn ergeben. Hie ein Beispiel dazu: _'Die Fliesen in meiner Küche sind violett, also ist es in Ordnung, wenn Sie diese Babys ertrinken lassen.'_ Es ist tatsächlich schwer, diese Äusserung als etwas anderes als einen Witz oder eine Metapher zu interpretieren. Wenn jemand ernsthaft versuchen würde, darauf zu drängen, würden wir sicherlich verlangen, über die Brückenprämisse zwischen Linoleumfarbe und nautischem Kindermord informiert zu werden, und wir wären skeptisch, dass irgendetwas Plausibles geliefert werden könnte [vgl. @riniMoralityCognitiveScience2015].

Kehren wir nun zu unserer eigentlichen Frage zurück: Ist es legitim, unsere moralischen Urteile aufgrund deskriptiver Aussagen zu verändern? Oder anders gesagt: sollten neurowissenschaftliche Erkenntnisse über den Ursprung unserer moralischen Urteile unsere moralischen Urteile kausal beeinflussen? Nehmen wir folgende Aussage: _'Gehirnareal B ist immer dann besonders aktiv, wenn man es als moralisch falsch erachtet, bei den Steuern zu betrügen. Es ist also moralisch falsch, bei seinen Steuern zu betrügen.'_ Was sollen wir von dieser Behauptung halten? Die scheinbar implizite Brückenprämisse lautet: _'Wenn Hirnareal B aktiv ist, wenn man X als falsch beurteilt, dann ist es falsch, X zu tun'._ Das scheint aber eine etwas spezielle Brückenprämisse zu sein; sie bezieht sich nicht auf interne Merkmale des Steuerbetrugs, die seine Falschheit erklären könnten. Tatsächlich scheint die Prämisse darauf hinzudeuten, dass eine Handlung falsch sein kann, nur weil jemand denkt, sie sei falsch, und im Körper dieser Person eine gewisse neuronale Aktivität stattfindet. Das ist aber nicht, was wir normalerweise tun, um zu moralischen Schlussfolgerungen zu gelangen und es ist unklar, weshalb wir das überzeugend finden sollten [vgl. @riniMoralityCognitiveScience2015].
Laut Berker [-@berkerNormativeInsignificanceNeuroscience2009] mangelt es Joshua Greenes neurowissenschaftlicher Entlarvung der Deontologie (siehe oben) an einer überzeugenden Brückenprämisse. Berker meint, Greene vermeide es, diese Prämisse explizit zu formulieren, denn wenn sie explizit gemacht würde, wäre sie entweder ein traditionelles moralisches Argument, das die Kognitionswissenschaft nicht zur Begründung seiner Schlussfolgerung heranzieht, oder sie würde kognitionswissenschaftliche Behauptungen verwenden, was aber zu keiner plausiblen moralischen Schlussfolgerung führen würde. Daher, so Berker, sei die Neurowissenschaft normativ unbedeutsam; sie spiele keine Rolle bei einer plausiblen Brückenprämisse zu einer moralischen Schlussfolgerung.
Natürlich impliziert das nur, dass Greenes Argument versagt. Aber Berker und andere Philosophen und Philosophinnen haben Zweifel daran geäussert, dass irgendein kognitionswissenschaftlich fundiertes Argument eine moralische Schlussfolgerung erzeugen könnte.Denn, wenn es ein erfolgreiches Brückenprinzip gäbe, wäre es ein sehr ungewöhnliches. Warum könnte die Tatsache, dass dieser oder jener kausale Prozess einem moralischen Urteil vorausgeht oder es begleitet, uns Anlass geben, unsere Meinung über dieses moralische Urteil zu ändern? Synchrone kausale Prozesse scheinen moralisch nicht relevant zu sein. Was Ihr Gehirn tut, während Sie moralische Urteile fällen, scheint doch eher in die gleiche Kategorie zu fallen wie die Farbe Ihrer Küchenfliesen - warum sollte das von Bedeutung sein?

## Moralische Kognition und moralische Epistemologie

Viele Moralphilosophen und Moralphilosophinnen betrachten moralische Urteile (oder Intuitionen) als Beweisstücke bei der Konstruktion von Moraltheorien. Rivalisierende moralische Prinzipien, wie z.B. diejenigen, die Deontologie und Konsequentialismus ausmachen, werden daraufhin geprüft, ob sie in bestimmten wichtigen Fällen (z.B. den Trolley-Dilemmata) Handlungsanweisungen geben, die unseren Intuitionen entsprechen. Ist dies nicht der Fall, wird das als Argument gegen diese Theorie verstanden. 
Diese Art der Argumentation steht im Mittelpunkt der normativen Ethik, wie sie von den meisten Philosophen und Philosophinnen praktiziert wird. Moralische Prinzipien werden vorgeschlagen, um intuitive Reaktionen auf bestimmte Szenarien zu erklären. Die besten Prinzipien sind diejenigen, die mit der grössten Anzahl relevanter Intuitionen übereinstimmen. Ein/e Philosoph/in, der/die ein Prinzip in Frage stellen möchte, wird ein kluges Gegenbeispiel konstruieren: ein Szenario, in dem es offensichtlich scheint, dass es falsch ist, X zu tun, aber das angestrebte Prinzip erlaubt es uns, X in diesem Fall zu tun. Die Befürworter des Prinzips haben nun vier Möglichkeiten:
1. zeigen, dass ihr Prinzip falsch angewendet wurde und tatsächlich ein anderes Urteil über den Fall fällt; 
2. akzeptieren, dass das Prinzip falsche Anweisungen liefert, es aber ändern, um eine bessere Antwort zu geben; 
3. in den sauren Apfel beißen und darauf bestehen, dass, selbst wenn das Prinzip hier falsch zu sein scheint, es immer noch vertrauenswürdig ist, weil es in so vielen anderen Fällen richtig ist; oder 
4. die problematische Intuition wegerklären, indem sie zeigen, dass der Testfall unterschätzt wird oder irgendwie unfair ist oder dass die intuitive Reaktion selbst wahrscheinlich aus einer Verwirrung resultiert. 
Diese Praxis stützt sich auf die vom Philosophen John Rawls [-@rawlsOutlineDecisionProcedure1951; -@rawlsTheoryJustice1999] vorgeschlagene Theorie des _reflective equilibrim_ (RE). Obwohl der Begriff _reflective equilibrium_ bei Rawls erstmals auftaucht, findet sich die Idee eines Gleichgewichts zwischen Regeln und Schlussfolgerungen und die Notwendigkeit, gegenseitige Anpassungen vorzunehmen, bereits in Goodman [-@goodmanFactFictionForecast1983]. Heutzutage ist das RE in vielen Zweigen der Philosophie eine beliebte Rechtfertigungsmethode, aber es erweist sich als schwierig, eine detaillierte Darstellung davon zu finden [vgl. @beisbartMakingItPreciseInBegutachtung, 2]. Die Idee scheint zu sein, von wohlüberlegten Urteilen (_considered judgments_) zu einem Thema auszugehen und diese dann mit einer Reihe von Prinzipien zu begründen. Sobald man eine Reihe von Prinzipien gefunden hat, die den wohlüberlegten Urteilen ziemlich gut entsprechen, überdenkt man die Urteile und Prinzipien und passt sie einander an, damit sie noch besser zueinander passen. Indem man zwischen den Urteilen und Prinzipien hin- und hergeht und sie gegenseitig aufeinander abstimmt, erreicht man ein Gleichgewicht, in dem Urteile und Prinzipien kohärent sind und sich gegenseitig stützen. Dieses Gleichgewicht nennt man _Überlegungsgleichgewicht_ oder _reflective equilibrium_. Alle unsere Überzeugungen, die an diesem _reflective equilibrium_ beteiligt sind, sind dann gerechtfertigt. Bei Moraltheorien werden Intuitionen über Testszenarien als Beweise für moralische Wahrheit betrachtet, ähnlich wie wissenschaftliche Beobachtungen Beweise für empirische Wahrheit sind. In den Naturwissenschaften hängt unser Vertrauen in eine bestimmte Theorie davon ab, ob sie durch wiederholte Beobachtungen beweiskräftige Unterstützung erhält, und in der Moralphilosophie hängt unser Vertrauen in eine bestimmte ethische Theorie davon ab, ob sie durch mehrere Intuitionen beweiskräftige Bestätigung erhält. #Kommentar: Peter Singer hier einbringen?

Diese Parallele zwischen Naturwissenschaft und Moralphilosophie zeigt uns eine Möglichkeit auf, wie die Psychologie des moralischen Urteilens für die Moralphilosophie relevant sein könnte. Wenn wir eine wissenschaftliche Theorie testen, hängt unser Vertrauen in jede experimentelle Beobachtung von unserem Vertrauen in die wissenschaftlichen Instrumente ab, die sie ermöglicht haben. Wenn wir an der Zuverlässigkeit unserer Instrumente zweifeln, dann sollten wir an den Beobachtungen zweifeln, die wir von ihnen erhalten, und ebenso sollten wir an den Theorien zweifeln, die sie zu stützen scheinen. Was also, wenn wir an der Verlässlichkeit unserer Instrumente in der Moralphilosophie zweifeln? Unsere Instrumente sind nichts anderes als unser eigener Verstand - oder genauer gesagt, unsere geistigen Fähigkeiten, Situationen zu verstehen und moralische Konzepte auf sie anzuwenden.




Niemand behauptet hier, dass die Kognitionswissenschaft uns direkt sagt, welche moralischen Urteile richtig und welche falsch sind. Alles, was die Kognitionswissenschaft tun kann, ist uns zu zeigen, dass bestimmte Intuitionen von bestimmten kausalen Faktoren beeinflusst werden - sie kann uns nicht sagen, welche kausalen Faktoren als verzerrend gelten und welche akzeptabel sind. Es liegt an uns, als moralische Richter, festzustellen, dass Unterschiede in der verbalen Rahmung (retten/sterben) oder der Schreibtischsauberkeit nicht zu echten moralischen Unterschieden führen. Natürlich müssen wir nicht sehr genau nachdenken, um zu entscheiden, dass diese kausalen Faktoren moralisch irrelevant sind - aber der Punkt bleibt, dass wir immer noch moralische Urteile fällen, selbst sehr einfache, und unser Urteil nicht direkt von der Kognitionswissenschaft erhalten.

Viele Befürworter einer Rolle der Kognitionswissenschaft in der Moral sind bereit, so viel zuzugestehen: Am Ende wird jede Revision unserer moralischen Urteile nur durch einige andere moralische Urteile autorisiert werden, nicht durch die Wissenschaft selbst. Aber sie werden jetzt darauf hinweisen, dass es einige Revisionen unserer moralischen Urteile gibt, die wir vornehmen sollten, und wir sind nur aufgrund der Beiträge der Kognitionswissenschaft in der Lage, diese vorzunehmen. Wir sind uns alle einig, dass unsere moralischen Urteile nicht durch die Sauberkeit unserer Umwelt beeinflusst werden sollten, und die Wissenschaft ist unnötig, damit wir uns darauf einigen können. Aber ohne die kognitive Wissenschaft wüssten wir nicht, dass unsere moralischen Urteile von der Sauberkeit der Umwelt beeinflusst werden. Zumindest in diesem Sinne scheint also die Verbesserung der Qualität unserer moralischen Urteile den Einsatz der kognitiven Wissenschaft zu erfordern. Positiv ausgedrückt: Wenn wir der kognitiven Wissenschaft der Moral Aufmerksamkeit schenken, können wir erkennen, dass einige scheinbar zuverlässige Intuitionen in Wirklichkeit nicht zuverlässig sind. Sobald diese wie zerbrochene Mikroskope beiseite gelegt werden, können wir größere Zuversicht haben, dass die Theorien, die wir aus dem Rest aufbauen, die moralische Wahrheit erfassen werden. (Siehe zum Beispiel Sinnott-Armstrong 2008; Mason 2011).

## Sense of agency

Es gibt eine andere Art und Weise, in der die Kognitionswissenschaft für die kohärente rationale Handlungskonzeption der Moral von Bedeutung sein kann. Einige Erkenntnisse der Kognitionswissenschaft können die Verständlichkeit dieser Konzeption insgesamt bedrohen. Erinnern Sie sich an die Arbeit des Psychologen Jonathan Haidt über moral dumbfounding; Leute scheinen spontan Rechtfertigungen für ihre intuitiven moralischen Urteile zu erfinden und halten auch dann noch an diesen Urteilen fest, wenn sich herausstellt, dass ihre Rechtfertigungen scheitern. In Haidts Interpretation zeigen diese Befunde, dass moralische Urteile fast völlig unbewusst geschehen, wobei bewusste moralische Argumentation meist ein Post-Hoc-Epiphänomen ist.

Wenn Haidt Recht hat, weisen Jeanette Kennett und Cordelia Fine (2009) darauf hin, dann stellt dies ein ernstes Problem für das Ideal der moralischen Handlungsfähigkeit dar. Damit wir als moralische Akteure gelten können, muss es die richtige Art von Verbindung zwischen unseren bewussten Überlegungen und unseren Reaktionen auf die Welt geben. Ein Roboter oder ein einfaches Tier kann reagieren, aber ein rationaler Akteur ist jemand, der kritisch über seine Gründe für sein Handeln nachdenken und zu einer wohlüberlegten Schlussfolgerung darüber kommen kann, was er tun sollte. Wenn wir jedoch in der von Haidt vorgeschlagenen Weise moralisch verblüfft sind, dann fehlt unserer bewussten moralischen Argumentation möglicherweise die angemessene Verbindung zu unseren moralischen Reaktionen. Wir glauben zu wissen, warum wir urteilen und handeln, wie wir es tun, aber in Wirklichkeit sind die Gründe, denen wir bewusst zustimmen, bloße Post-Hoc-Erfindungen.

Am Ende argumentieren Kennett und Fine, dass Haidts Ergebnisse nicht wirklich zu dieser unwillkommenen Schlussfolgerung führen. Sie legen nahe, dass er das, was die Experimente zeigen, falsch interpretiert hat und dass es eine plausiblere Interpretation gibt, die die Möglichkeit bewusster moralischer Handlungsfähigkeit bewahrt.

Ein möglicher Ansatz ist die selektive Anwendung von Haidts Argumentation. Wenn gezeigt werden könnte, dass bestimmte moralische Urteile - zu einem bestimmten Thema oder Teilbereich der Moral - besonders anfällig für _moral dumbfounding_ sind, dann könnten wir die Grundlage dafür haben, sie von der Einbeziehung in die reflektierende Moraltheorie auszuschließen. Dies scheint bisweilen der Ansatz zu sein, den Joshua Greene in seinem psychologischen Angriff auf die Deontologie gewählt hat. Nach Greene (2014) sind deontologische Intuitionen von einem psychologischen Typus, der deutlich von bewusster Reflexion abgekoppelt ist und dementsprechend misstrauisch betrachtet werden sollte. Viele Philosophen bestreiten Greenes Behauptungen (siehe z.B. Kahane 2012), aber diese Debatte selbst zeigt den Reichtum der Auseinandersetzung zwischen Ethik und Kognitionswissenschaft


# Konklusion




